{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "!pip install jax\n",
    "!pip install jaxopt\n",
    "!pip install mxnet"
   ],
   "execution_count":1,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Requirement already satisfied: jax in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (0.4.6)\r\n",
      "Requirement already satisfied: scipy>=1.5 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jax) (1.10.1)\r\n",
      "Requirement already satisfied: numpy>=1.20 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jax) (1.23.5)\r\n",
      "Requirement already satisfied: opt-einsum in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jax) (3.3.0)\r\n",
      "Requirement already satisfied: jaxopt in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (0.6)\r\n",
      "Requirement already satisfied: numpy>=1.18.4 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jaxopt) (1.23.5)\r\n",
      "Requirement already satisfied: jax>=0.2.18 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jaxopt) (0.4.6)\r\n",
      "Requirement already satisfied: jaxlib>=0.1.69 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jaxopt) (0.4.6)\r\n",
      "Requirement already satisfied: scipy>=1.0.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jaxopt) (1.10.1)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jaxopt) (1.4.0)\r\n",
      "Requirement already satisfied: matplotlib>=2.0.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jaxopt) (3.5.1)\r\n",
      "Requirement already satisfied: opt-einsum in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from jax>=0.2.18->jaxopt) (3.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib>=2.0.1->jaxopt) (3.0.9)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib>=2.0.1->jaxopt) (9.4.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib>=2.0.1->jaxopt) (23.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib>=2.0.1->jaxopt) (2.8.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib>=2.0.1->jaxopt) (4.39.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib>=2.0.1->jaxopt) (0.11.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib>=2.0.1->jaxopt) (1.4.4)\r\n",
      "Requirement already satisfied: six>=1.5 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from python-dateutil>=2.7->matplotlib>=2.0.1->jaxopt) (1.16.0)\r\n",
      "Requirement already satisfied: mxnet in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (1.9.1)\r\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from mxnet) (0.8.4)\r\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from mxnet) (2.26.0)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from mxnet) (1.23.5)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests<3,>=2.20.0->mxnet) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests<3,>=2.20.0->mxnet) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests<3,>=2.20.0->mxnet) (2.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\r\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"t4vxJTVRBjKRt091Ec7qRs",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "**Backpropagation в PyTorch CPU**"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"tHd0xn3blRabdKMVW3cpkt",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Определяем параметры модели\n",
    "input_size = 2\n",
    "hidden_size = 10\n",
    "output_size = 2\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Создаем модель\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Создаем наборы данных\n",
    "X = torch.tensor([[0.2, 0.1], [0.4, 0.7], [0.3, 0.5], [0.7, 0.9], [0.8, 0.4], [0.6, 0.2]], dtype=torch.float32)\n",
    "y = torch.tensor([0, 1, 1, 0, 0, 1], dtype=torch.long)\n",
    "\n",
    "# Создаем наборы данных для загрузки в DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Создаем экземпляр модели и функцию потерь и определяем оптимизатор\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)\n",
    "\n",
    "# Обучаем модель\n",
    "start_time = time.time()\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {running_loss \/ len(dataset):.6f}\")\n",
    "    elif epoch == 999:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {running_loss \/ len(dataset):.6f}\")\n",
    "end_time = time.time()\n",
    "\n",
    "# Проверяем результаты\n",
    "with torch.no_grad():\n",
    "    outputs = model(X)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(\"Predicted labels:\", predicted)\n",
    "\n",
    "print(f'total training time {end_time - start_time}')"
   ],
   "execution_count":2,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Epoch 0 | Loss: 0.357817\n",
      "Epoch 100 | Loss: 0.341468\n",
      "Epoch 200 | Loss: 0.336804\n",
      "Epoch 300 | Loss: 0.329916\n",
      "Epoch 400 | Loss: 0.322687\n",
      "Epoch 500 | Loss: 0.312558\n",
      "Epoch 600 | Loss: 0.302015\n",
      "Epoch 700 | Loss: 0.290985\n",
      "Epoch 800 | Loss: 0.276702\n",
      "Epoch 900 | Loss: 0.262132\n",
      "Epoch 1000 | Loss: 0.242933\n",
      "Predicted labels: tensor([1, 1, 1, 0, 0, 1])\n",
      "total training time 1.8945322036743164\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"K30QTJYQFMmBLI8SAloZ8Q",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "**Backpropagation в TensorFlow CPU**"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"K8Av0lSqytCj1lLBo8xbSp",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "with tf.device('cpu'):\n",
    "    X = [[0.2, 0.1], [0.4, 0.7], [0.3, 0.5], [0.7, 0.9], [0.8, 0.4], [0.6, 0.2]]\n",
    "    y = [0, 1, 1, 0, 0, 1]\n",
    "\n",
    "    num_hidden = 10\n",
    "    num_epochs = 1000\n",
    "    batch_size = 2\n",
    "\n",
    "    # Создаем граф вычислений\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Определяем входные данные\n",
    "    X_placeholder = tf.keras.layers.Input(shape=(2,), name='input')\n",
    "\n",
    "    # Определяем веса\n",
    "    W1 = tf.Variable(tf.random.normal([2, num_hidden]), name=\"W1\")\n",
    "    W2 = tf.Variable(tf.random.normal([num_hidden, 1]), name=\"W2\")\n",
    "\n",
    "    # Определяем смещения\n",
    "    b1 = tf.Variable(tf.zeros([num_hidden]), name=\"b1\")\n",
    "    b2 = tf.Variable(tf.zeros([1]), name=\"b2\")\n",
    "\n",
    "    # Определяем скрытый слой\n",
    "    hidden_layer = tf.nn.sigmoid(tf.matmul(X_placeholder, W1) + b1)\n",
    "\n",
    "    # Определяем выходной слой\n",
    "    output_layer = tf.nn.sigmoid(tf.matmul(hidden_layer, W2) + b2)\n",
    "\n",
    "    # Определяем функцию потерь\n",
    "    loss = tf.reduce_mean(tf.square(output_layer - y))\n",
    "\n",
    "    # Определяем оптимизатор\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "    # Определяем модель\n",
    "    model = tf.keras.Model(inputs=X_placeholder, outputs=output_layer)\n",
    "\n",
    "    # Компилируем модель\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    # Обучаем модель\n",
    "    start_time = time.time()\n",
    "    model.fit(x=X, y=y, epochs=num_epochs, batch_size=batch_size, verbose=0)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Получаем предсказания\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    print(predictions[0])\n",
    "    for i in range(len(X)):\n",
    "        print(f'input sample: {X[i]}, probability: {predictions[i][0]}')\n",
    "\n",
    "    print(f'total training time {end_time - start_time}')"
   ],
   "execution_count":3,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'W1:0' shape=(2, 10) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'b1:0' shape=(10,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_1), but are not present in its tracked objects:   <tf.Variable 'W2:0' shape=(10, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'b2:0' shape=(1,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 104ms\/step\n",
      "[0.5085831]\n",
      "input sample: [0.2, 0.1], probability: 0.508583128452301\n",
      "input sample: [0.4, 0.7], probability: 0.679873526096344\n",
      "input sample: [0.3, 0.5], probability: 0.6171969175338745\n",
      "input sample: [0.7, 0.9], probability: 0.7931803464889526\n",
      "input sample: [0.8, 0.4], probability: 0.7907161712646484\n",
      "input sample: [0.6, 0.2], probability: 0.7056461572647095\n",
      "total training time 23.718749523162842\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"d9cvwtbf67Zs0uUM0ZREew",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "**Backpropagation в JAX CPU**"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"ZfA2yLJSewTV4ax4AXcJEc",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "jax.devices('cpu')\n",
    "\n",
    "# Определяем sigmoid функцию\n",
    "def sigmoid(x):\n",
    "    return 1 \/ (1 + jnp.exp(-x))\n",
    "\n",
    "# Опредделяем параметры нейросети\n",
    "num_hidden = 10\n",
    "key = random.PRNGKey(0)\n",
    "W1 = random.normal(key, (2, num_hidden))\n",
    "W2 = random.normal(key, (num_hidden, 1))\n",
    "b1 = jnp.zeros((num_hidden,))\n",
    "b2 = jnp.zeros((1,))\n",
    "\n",
    "# Определяем функцию потерь\n",
    "def loss(params, x, y):\n",
    "    W1, b1, W2, b2 = params\n",
    "    hidden_layer = sigmoid(jnp.dot(x, W1) + b1)\n",
    "    output_layer = sigmoid(jnp.dot(hidden_layer, W2) + b2)\n",
    "    return jnp.mean(jnp.square(output_layer - y))\n",
    "\n",
    "# Определяем оптимизатор\n",
    "def update(params, x, y, lr=0.01):\n",
    "    grads = jax.grad(loss)(params, x, y)\n",
    "    return [param - lr * grad for param, grad in zip(params, grads)]\n",
    "\n",
    "# Тренируем модель\n",
    "X = jnp.array([[0.2, 0.1], [0.4, 0.7], [0.3, 0.5], [0.7, 0.9], [0.8, 0.4], [0.6, 0.2]])\n",
    "y = jnp.array([0, 1, 1, 0, 0, 1])\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "num_epochs = 1000\n",
    "batch_size = 2\n",
    "lr = 0.01\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_X = X[i:i+batch_size]\n",
    "        batch_y = y[i:i+batch_size]\n",
    "        params = update(params, batch_X, batch_y, lr=lr)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}, loss {loss(params, X, y):.6f}\")\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "predictions = sigmoid(jnp.dot(sigmoid(jnp.dot(X, W1) + b1), W2) + b2)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input sample: {X[i]}, probability: {predictions[i][0]:.6f}\")\n",
    "\n",
    "print(f\"Total training time {end_time - start_time:.3f} seconds\")"
   ],
   "execution_count":4,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Epoch 100, loss 0.251842\n",
      "Epoch 200, loss 0.251176\n",
      "Epoch 300, loss 0.251122\n",
      "Epoch 400, loss 0.251094\n",
      "Epoch 500, loss 0.251067\n",
      "Epoch 600, loss 0.251042\n",
      "Epoch 700, loss 0.251017\n",
      "Epoch 800, loss 0.250993\n",
      "Epoch 900, loss 0.250970\n",
      "Epoch 1000, loss 0.250947\n",
      "Input sample: [0.2 0.1], probability: 0.330406\n",
      "Input sample: [0.4 0.7], probability: 0.392539\n",
      "Input sample: [0.3 0.5], probability: 0.374074\n",
      "Input sample: [0.7 0.9], probability: 0.419339\n",
      "Input sample: [0.8 0.4], probability: 0.390050\n",
      "Input sample: [0.6 0.2], probability: 0.359054\n",
      "Total training time 76.224 seconds\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "No GPU\/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"DivNF6PVxduVXZb0PsMCrp",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "**Backpropagation в MXNet CPU**"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Lqqdg0nsZEZ2WSPsItiAke",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "\n",
    "# Определяем параметры\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "batch_size = 2\n",
    "num_inputs = 2\n",
    "num_outputs = 1\n",
    "num_hidden = 10\n",
    "\n",
    "X = nd.array([[0.2, 0.1], [0.4, 0.7], [0.3, 0.5], [0.7, 0.9], [0.8, 0.4], [0.6, 0.2]])\n",
    "y = nd.array([0, 1, 1, 0, 0, 1])\n",
    "\n",
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset(X, y), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Определяем модель нейросети\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs, activation=\"sigmoid\"))\n",
    "net.initialize()\n",
    "\n",
    "# Определяем функцию потерь\n",
    "loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "# Определяем оптимизатор\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': learning_rate})\n",
    "\n",
    "# Тренируем модель\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            L = loss(output, label)\n",
    "        L.backward()\n",
    "        trainer.step(batch_size)\n",
    "        cumulative_loss += nd.sum(L).asscalar()\n",
    "    avg_loss = cumulative_loss \/ X.shape[0]\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {avg_loss}\")\n",
    "    elif epoch == 999:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss}\")\n",
    "end_time = time.time()\n",
    "\n",
    "predicted_classes = (net(X) > 0.5).reshape((-1,))\n",
    "for i in range(len(X)):\n",
    "    print(f'input sample: {X.asnumpy()[i]}, probability: {predicted_classes.asnumpy()[i]}')\n",
    "\n",
    "print(f'total training time {end_time - start_time}')"
   ],
   "execution_count":5,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Epoch 0 | Loss: 0.725563128789266\n",
      "Epoch 100 | Loss: 0.6928676764170328\n",
      "Epoch 200 | Loss: 0.6911760369936625\n",
      "Epoch 300 | Loss: 0.68854288260142\n",
      "Epoch 400 | Loss: 0.6824089686075846\n",
      "Epoch 500 | Loss: 0.6671608686447144\n",
      "Epoch 600 | Loss: 0.6437016924222311\n",
      "Epoch 700 | Loss: 0.6202540000279745\n",
      "Epoch 800 | Loss: 0.6016271511713663\n",
      "Epoch 900 | Loss: 0.5898056030273438\n",
      "Epoch 1000 | Loss: 0.5825513402620951\n",
      "input sample: [0.2 0.1], probability: 0.0\n",
      "input sample: [0.4 0.7], probability: 1.0\n",
      "input sample: [0.3 0.5], probability: 1.0\n",
      "input sample: [0.7 0.9], probability: 0.0\n",
      "input sample: [0.8 0.4], probability: 0.0\n",
      "input sample: [0.6 0.2], probability: 0.0\n",
      "total training time 9.975848197937012\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"FNk6g35rPxqjSKlAfr5Zs9",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ],
   "report_row_ids":[
    
   ],
   "version":2
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}